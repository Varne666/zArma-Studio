const express = require('express');
const cors = require('cors');
const axios = require('axios');
const fs = require('fs');
const path = require('path');
const os = require('os');
const { execFile, spawn } = require('child_process');
const http = require('http');
const https = require('https');
const crypto = require('crypto');
const zlib = require('zlib');
const { promisify } = require('util');

const fsPromises = fs.promises;
const gzip = promisify(zlib.gzip);

const app = express();
const PORT = process.env.PORT || 3001;
const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';
const DEFAULT_MODEL = process.env.DEFAULT_MODEL || 'nous-hermes:13b';

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PERFORMANCE OPTIMIZATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Connection pooling for Ollama
const ollamaHttpAgent = new http.Agent({
  keepAlive: true,
  maxSockets: 10,
  maxFreeSockets: 5,
  timeout: 60000,
  freeSocketTimeout: 30000
});

// 2. In-memory cache with TTL
class Cache {
  constructor(defaultTTL = 300000) {
    this.store = new Map();
    this.defaultTTL = defaultTTL;
  }

  get(key) {
    const item = this.store.get(key);
    if (!item) return null;
    if (Date.now() > item.expires) {
      this.store.delete(key);
      return null;
    }
    return item.value;
  }

  set(key, value, ttl = this.defaultTTL) {
    this.store.set(key, {
      value,
      expires: Date.now() + ttl
    });
  }

  delete(key) {
    this.store.delete(key);
  }

  clear() {
    this.store.clear();
  }

  cleanup() {
    const now = Date.now();
    for (const [key, item] of this.store) {
      if (now > item.expires) {
        this.store.delete(key);
      }
    }
  }
}

const ollamaCache = new Cache(300000); // 5 minutes TTL
const healthCache = new Cache(10000); // 10 seconds TTL for health checks

// 3. Request deduplication
const inFlightRequests = new Map();

async function dedupRequest(cacheKey, requestFn) {
  const existing = inFlightRequests.get(cacheKey);
  if (existing) {
    return existing;
  }

  const promise = requestFn().finally(() => {
    inFlightRequests.delete(cacheKey);
  });

  inFlightRequests.set(cacheKey, promise);
  return promise;
}

// 4. Simple in-memory rate limiter
class RateLimiter {
  constructor(windowMs = 60000, maxRequests = 100) {
    this.windowMs = windowMs;
    this.maxRequests = maxRequests;
    this.requests = new Map();
  }

  isAllowed(clientId) {
    const now = Date.now();
    const clientRequests = this.requests.get(clientId);

    if (!clientRequests) {
      this.requests.set(clientId, { count: 1, resetTime: now + this.windowMs });
      return { allowed: true, remaining: this.maxRequests - 1 };
    }

    if (now > clientRequests.resetTime) {
      clientRequests.count = 1;
      clientRequests.resetTime = now + this.windowMs;
      return { allowed: true, remaining: this.maxRequests - 1 };
    }

    if (clientRequests.count >= this.maxRequests) {
      return { 
        allowed: false, 
        remaining: 0, 
        retryAfter: Math.ceil((clientRequests.resetTime - now) / 1000) 
      };
    }

    clientRequests.count++;
    return { allowed: true, remaining: this.maxRequests - clientRequests.count };
  }

  cleanup() {
    const now = Date.now();
    for (const [clientId, data] of this.requests) {
      if (now > data.resetTime) {
        this.requests.delete(clientId);
      }
    }
  }
}

const rateLimiter = new RateLimiter(60000, 60); // 60 requests per minute

// 5. Compression middleware (manual since no external compression lib)
async function compressResponse(req, res, data, minSize = 1024) {
  const acceptEncoding = req.headers['accept-encoding'] || '';
  
  if (typeof data !== 'string' || data.length < minSize || !acceptEncoding.includes('gzip')) {
    return { compressed: false, data };
  }

  try {
    const compressed = await gzip(Buffer.from(data, 'utf8'));
    return { compressed: true, data: compressed, encoding: 'gzip' };
  } catch {
    return { compressed: false, data };
  }
}

// 6. Fast hash function for cache keys
function fastHash(str) {
  return crypto.createHash('md5').update(str).digest('hex');
}

// 7. Async JSON parse/stringify wrapper with size limit for optimization
function fastParseJSON(json, defaultValue = null) {
  try {
    if (json.length > 1000000) {
      // For large JSON, use streaming approach via manual parsing
      return JSON.parse(json);
    }
    return JSON.parse(json);
  } catch {
    return defaultValue;
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MIDDLEWARE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app.use(cors());
app.use(express.json({ limit: '100mb' }));

// Rate limiting middleware
app.use((req, res, next) => {
  const clientId = req.ip || req.connection.remoteAddress || 'unknown';
  const result = rateLimiter.isAllowed(clientId);

  if (!result.allowed) {
    return res.status(429).json({
      error: 'Too many requests',
      retryAfter: result.retryAfter
    });
  }

  res.setHeader('X-RateLimit-Remaining', result.remaining);
  next();
});

// Response compression middleware for JSON responses
app.use((req, res, next) => {
  const originalJson = res.json.bind(res);
  
  res.json = async function(data) {
    const jsonString = JSON.stringify(data);
    const compression = await compressResponse(req, res, jsonString);
    
    if (compression.compressed) {
      res.setHeader('Content-Encoding', compression.encoding);
      res.setHeader('Content-Type', 'application/json');
      return res.send(compression.data);
    }
    
    return originalJson(data);
  };
  
  next();
});

// Determine if running from Electron
const isPackaged = fs.existsSync(path.join(__dirname, '../app/dist'));

// Static files path
let staticPath;
if (isPackaged) {
  staticPath = path.join(__dirname, '../app/dist');
} else {
  staticPath = path.join(__dirname, '../dist');
}

console.log('Looking for static files at:', staticPath);
console.log('Directory exists:', fs.existsSync(staticPath));

if (fs.existsSync(staticPath)) {
  app.use(express.static(staticPath, {
    maxAge: '1h',
    etag: true,
    lastModified: true
  }));
  console.log('âœ“ Serving static files from:', staticPath);
} else {
  console.error('âœ— Static path not found:', staticPath);
}

// Data directory
const DATA_DIR = path.join(os.homedir(), '.zimage-prompts', 'data');
console.log('Data directory:', DATA_DIR);

(async () => {
  try {
    await fsPromises.access(DATA_DIR).catch(async () => {
      await fsPromises.mkdir(DATA_DIR, { recursive: true });
      console.log('Created data directory');
    });
  } catch (err) {
    console.error('Failed to create data directory:', err);
  }
})();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AUTO CLEANUP - Removes temp files but keeps saved prompts
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function cleanupTempFilesAsync() {
  const tmpDir = os.tmpdir();
  const patterns = ['armscaler_*', 'zimage_*', 'input_*.png', 'output_*.png'];
  
  let cleaned = 0;
  let freedBytes = 0;
  
  try {
    const files = await fsPromises.readdir(tmpDir);
    const cleanupPromises = [];
    
    for (const file of files) {
      for (const pattern of patterns) {
        const regex = new RegExp(pattern.replace('*', '.*'));
        if (regex.test(file)) {
          const filePath = path.join(tmpDir, file);
          cleanupPromises.push(
            (async () => {
              try {
                const stats = await fsPromises.stat(filePath);
                if (stats.isDirectory()) {
                  await fsPromises.rm(filePath, { recursive: true, force: true });
                } else {
                  freedBytes += stats.size;
                  await fsPromises.unlink(filePath);
                }
                cleaned++;
              } catch (e) {
                // File might be in use, skip
              }
            })()
          );
          break;
        }
      }
    }
    
    await Promise.allSettled(cleanupPromises);
    
    if (cleaned > 0) {
      const mb = (freedBytes / 1024 / 1024).toFixed(1);
      console.log(`ðŸ§¹ Cleaned ${cleaned} temp files, freed ~${mb} MB`);
    }
  } catch (err) {
    console.error('Cleanup error:', err.message);
  }
}

// Cleanup on startup
cleanupTempFilesAsync();

// Cleanup every 30 minutes
setInterval(cleanupTempFilesAsync, 30 * 60 * 1000);

// Cleanup endpoint (manual trigger)
app.post('/api/cleanup', async (req, res) => {
  await cleanupTempFilesAsync();
  res.json({ success: true, message: 'Cleanup completed' });
});

// Check Python availability
let pythonCmd = null;
(async () => {
  for (const cmd of ['python3', 'python']) {
    try {
      await new Promise((resolve, reject) => {
        execFile(cmd, ['--version'], { timeout: 5000 }, (err) => {
          if (err) reject(err);
          else resolve();
        });
      });
      pythonCmd = cmd;
      console.log(`âœ“ Python available: ${cmd}`);
      break;
    } catch (e) {}
  }
  if (!pythonCmd) {
    console.log('âœ— Python not found');
  }
})();

app.get('/api/ollama-status', async (req, res) => {
  const cacheKey = 'ollama_status';
  const cached = healthCache.get(cacheKey);
  
  if (cached) {
    return res.json(cached);
  }

  try {
    const axiosInstance = axios.create({
      httpAgent: ollamaHttpAgent,
      timeout: 5000
    });
    const response = await axiosInstance.get(`${OLLAMA_URL}/api/tags`);
    const result = { status: 'connected', models: response.data.models || [] };
    healthCache.set(cacheKey, result);
    res.json(result);
  } catch (error) {
    const result = { status: 'disconnected', error: error.message };
    res.status(503).json(result);
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ARMscaler (DiffBIR) API
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const ARMSCALER_SCRIPT = path.join(__dirname, 'armscaler_simple.py');
const ARMSCALER_QUICK_STATUS = path.join(__dirname, 'armscaler_quick_status.py');
const DIFFBIR_DIR = path.join(__dirname, 'diffbir_engine');

// Simple status check (no persistent server)
app.get('/api/armscaler-status', async (req, res) => {
  try {
    if (!pythonCmd) {
      return res.json({
        available: false,
        message: 'Python not available',
        gpu: { available: false, name: 'Unknown', vram_gb: 0 }
      });
    }
    
    const result = await new Promise((resolve, reject) => {
      execFile(pythonCmd, [ARMSCALER_QUICK_STATUS], { timeout: 10000 }, (err, stdout) => {
        if (err) reject(err);
        else resolve(stdout.trim());
      });
    });
    
    const status = fastParseJSON(result, {
      available: false,
      message: 'Failed to parse status',
      gpu: { available: false, name: 'Unknown', vram_gb: 0 }
    });
    res.json(status);
  } catch (error) {
    console.error('ARMscaler status error:', error.message);
    const diffbirExists = fs.existsSync(path.join(DIFFBIR_DIR, 'inference.py'));
    res.json({
      available: diffbirExists,
      message: error.message,
      setup_complete: diffbirExists,
      gpu: { available: false, name: 'Unknown', vram_gb: 0 }
    });
  }
});

// Job tracking
const armScalerJobs = {};

// Start processing job
app.post('/api/armscaler/job', async (req, res) => {
  const { imageBase64, task = 'sr', upscale = 4, quality = 'balanced' } = req.body;
  if (!imageBase64) return res.status(400).json({ error: 'No image data' });
  
  const jobId = crypto.randomUUID();
  
  const job = {
    id: jobId,
    status: 'running',
    startTime: Date.now(),
    endTime: null,
    logs: [['info', 'Starting ARMscaler inference...']],
    resultBase64: null,
    error: null,
    progress: 10
  };
  armScalerJobs[jobId] = job;
  
  // Process in background
  const tmpDir = path.join(os.tmpdir(), `armscaler_${jobId}`);
  await fsPromises.mkdir(tmpDir, { recursive: true });
  
  const inputPath = path.join(tmpDir, 'input.png');
  const outputPath = path.join(tmpDir, 'output.png');
  
  // Write input
  const base64Data = imageBase64.replace(/^data:image\/\w+;base64,/, '');
  await fsPromises.writeFile(inputPath, Buffer.from(base64Data, 'base64'));
  
  // Spawn process
  const env = {
    ...process.env,
    PYTHONPATH: `${DIFFBIR_DIR}:${process.env.PYTHONPATH || ''}`,
    PYTHONUNBUFFERED: '1',
    PYTORCH_CUDA_ALLOC_CONF: 'max_split_size_mb=512,expandable_segments=True',
    NVIDIA_TF32_OVERRIDE: '1'
  };
  
  const child = spawn(pythonCmd || 'python3', [
    ARMSCALER_SCRIPT,
    inputPath,
    outputPath,
    '--task', task,
    '--upscale', String(upscale),
    '--quality', quality
  ], { env });
  
  let stdoutBuffer = '';
  let stderrBuffer = '';
  
  child.stdout.on('data', (data) => {
    stdoutBuffer += data.toString();
    job.progress = Math.min(90, job.progress + 5);
    
    // Parse progress from output
    const lines = data.toString().split('\n');
    for (const line of lines) {
      if (line.includes('Server:')) {
        job.logs.push(['info', line.replace(/.*Server:\s*/, '')]);
      } else if (line.includes('inference') || line.includes('Processing')) {
        job.logs.push(['info', line.trim()]);
      }
    }
  });
  
  child.stderr.on('data', (data) => {
    stderrBuffer += data.toString();
    const lines = data.toString().split('\n');
    for (const line of lines) {
      if (line.trim()) {
        job.logs.push(['info', line.trim()]);
      }
    }
  });
  
  child.on('close', async (code) => {
    if (code === 0) {
      try {
        await fsPromises.access(outputPath);
        const buf = await fsPromises.readFile(outputPath);
        job.resultBase64 = buf.toString('base64');
        job.status = 'done';
        job.progress = 100;
        job.logs.push(['success', 'Processing complete']);
      } catch (e) {
        job.status = 'error';
        job.error = 'Failed to read output';
        job.logs.push(['error', 'Failed to read output file']);
      }
    } else {
      job.status = 'error';
      job.error = stderrBuffer.slice(-500) || 'Processing failed';
      job.logs.push(['error', 'Processing failed']);
    }
    
    job.endTime = Date.now();
    
    // Cleanup temp files
    try {
      await fsPromises.rm(tmpDir, { recursive: true, force: true });
    } catch (e) {}
  });
  
  res.json({ job_id: jobId });
});

app.get('/api/armscaler/job/:id', (req, res) => {
  const job = armScalerJobs[req.params.id];
  if (!job) return res.status(404).json({ error: 'Job not found' });
  
  res.json({
    id: job.id,
    status: job.status,
    progress: job.progress,
    logs: job.logs.map(([type, msg]) => `[${type}] ${msg}`),
    error: job.error,
    imageBase64: job.status === 'done' ? job.resultBase64 : undefined,
    startTime: job.startTime,
    endTime: job.endTime
  });
});

// Synchronous endpoint (for simple use)
app.post('/api/armscaler', async (req, res) => {
  const { imageBase64, task = 'sr', upscale = 4, quality = 'balanced' } = req.body;
  if (!imageBase64) return res.status(400).json({ error: 'No image data' });
  
  let tmpDir;
  try {
    // Create temp files
    tmpDir = path.join(os.tmpdir(), `armscaler_sync_${Date.now()}`);
    await fsPromises.mkdir(tmpDir, { recursive: true });
    
    const inputPath = path.join(tmpDir, 'input.png');
    const outputPath = path.join(tmpDir, 'output.png');
    
    // Write input
    const base64Data = imageBase64.replace(/^data:image\/\w+;base64,/, '');
    await fsPromises.writeFile(inputPath, Buffer.from(base64Data, 'base64'));
    
    // Run inference
    const env = {
      ...process.env,
      PYTHONPATH: `${DIFFBIR_DIR}:${process.env.PYTHONPATH || ''}`,
      PYTHONUNBUFFERED: '1',
      PYTORCH_CUDA_ALLOC_CONF: 'max_split_size_mb=512,expandable_segments=True',
      NVIDIA_TF32_OVERRIDE: '1'
    };
    
    await new Promise((resolve, reject) => {
      const child = spawn(pythonCmd || 'python3', [
        ARMSCALER_SCRIPT,
        inputPath,
        outputPath,
        '--task', task,
        '--upscale', String(upscale),
        '--quality', quality
      ], { env });
      
      let stderr = '';
      child.stderr.on('data', (d) => {
        stderr += d.toString();
        console.log(`[ARMscaler] ${d.toString().trim()}`);
      });
      
      child.on('close', async (code) => {
        try {
          await fsPromises.access(outputPath);
          if (code === 0) {
            resolve();
          } else {
            reject(new Error(stderr.slice(-500) || 'Processing failed'));
          }
        } catch {
          reject(new Error(stderr.slice(-500) || 'Processing failed'));
        }
      });
    });
    
    // Read result
    const buf = await fsPromises.readFile(outputPath);
    
    // Cleanup
    try { await fsPromises.rm(tmpDir, { recursive: true, force: true }); } catch (e) {}
    
    res.json({ imageBase64: buf.toString('base64'), engine: 'diffbir' });
    
  } catch (err) {
    console.error('ARMscaler error:', err);
    if (tmpDir) {
      try { await fsPromises.rm(tmpDir, { recursive: true, force: true }); } catch (e) {}
    }
    res.status(500).json({ error: err.message });
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Watermark Removal API
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const INPAINT_SCRIPT = path.join(__dirname, 'inpaint_lama.py');
const MODEL_PATH = path.join(__dirname, 'models', 'lama_fp32.onnx');

app.get('/api/inpaint-status', async (req, res) => {
  try {
    await fsPromises.access(MODEL_PATH);
    res.json({ available: pythonCmd !== null, modelPath: MODEL_PATH });
  } catch {
    res.json({ available: false, modelPath: MODEL_PATH });
  }
});

app.post('/api/inpaint', async (req, res) => {
  try {
    await fsPromises.access(MODEL_PATH);
  } catch {
    return res.status(503).json({ error: 'LaMa model not found' });
  }
  
  const { imageBase64, mask } = req.body;
  if (!imageBase64 || !mask) {
    return res.status(400).json({ error: 'Missing imageBase64 or mask' });
  }
  
  const tmpDir = path.join(os.tmpdir(), 'zimage-inpaint');
  try {
    await fsPromises.mkdir(tmpDir, { recursive: true });
    
    const ts = Date.now();
    const inputPath = path.join(tmpDir, `input_${ts}.png`);
    const outputPath = path.join(tmpDir, `output_${ts}.png`);
    
    const base64Data = imageBase64.replace(/^data:image\/\w+;base64,/, '');
    await fsPromises.writeFile(inputPath, Buffer.from(base64Data, 'base64'));
    
    const args = [
      INPAINT_SCRIPT,
      inputPath,
      outputPath,
      String(Math.round(mask.x)),
      String(Math.round(mask.y)),
      String(Math.round(mask.w)),
      String(Math.round(mask.h)),
      String(Math.round(mask.padding || 3)),
      MODEL_PATH
    ];
    
    await new Promise((resolve, reject) => {
      execFile(pythonCmd || 'python3', args, { timeout: 60000 }, (err) => {
        if (err) reject(err);
        else resolve();
      });
    });
    
    try {
      await fsPromises.access(outputPath);
    } catch {
      throw new Error('Output not created');
    }
    
    const resultBuffer = await fsPromises.readFile(outputPath);
    const resultBase64 = 'data:image/png;base64,' + resultBuffer.toString('base64');
    
    try { await fsPromises.unlink(inputPath); } catch(e) {}
    try { await fsPromises.unlink(outputPath); } catch(e) {}
    
    res.json({ imageBase64: resultBase64, engine: 'lama-onnx' });
    
  } catch (error) {
    console.error('Inpaint error:', error.message);
    res.status(500).json({ error: error.message });
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// OPTIMIZED SYSTEM PROMPTS - UNHINGED EDITION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const UNHINGED_SYSTEM_PROMPT = `You are Z-Image Turbo Prompt Engine v2. Generate Instagram-style image prompts for AI generation.

CRITICAL OUTPUT RULES:
- If PROSE requested: Output ONLY a single dense paragraph. NO JSON, NO bullet points, NO code blocks, NO "JSON:" or "PROSE:" prefixes.
- If JSON requested: Output ONLY valid JSON object. NO prose, NO explanations, NO markdown code blocks (no \`\`\`), NO text before or after the JSON.

STYLE: Amateur cellphone photography (iPhone/Android), grainy, crushed blacks, overexposed highlights, front-camera distortion, messy real environments, authentic imperfections.

SUBJECT: Eastern European women 18-25, FIT BODY with exaggerated hourglass (tiny waist, large breasts with 6-8" sagittal depth, large glutes with 6-8" depth), HORNY EXPRESSIONS (sultry half-lidded eyes, parted plump lips, flushed cheeks, breathy/bitten lip look, needy gaze), revealing but mostly SFW clothing (tight leggings, crop tops, mini skirts, low-cut tops - NO brand names, describe fabric physics: stretch, tension, compression).

FOR JSON - INCLUDE ALL SECTIONS:
- metadata: confidence_score, image_type, primary_purpose
- composition: rule_applied, aspect_ratio, layout, focal_points[], visual_hierarchy, balance
- color_profile: dominant_colors[] with hex codes (#RRGGBB) and percentages, color_palette, temperature, saturation, contrast
- lighting: type, source_count, direction, quality, intensity, contrast_ratio, mood, shadows{type,density,placement,length}, highlights{treatment,placement}, ambient_fill, light_temperature
- technical_specs: medium, style, texture, sharpness, grain, depth_of_field, perspective
- artistic_elements: genre, influences[], mood, atmosphere, visual_style
- typography: present, fonts[], placement, integration
- subject_analysis: primary_subject, positioning, scale, interaction, facial_expression{mouth,smile_intensity,eyes,eyebrows,overall_emotion,authenticity}, hair{length,cut,texture,texture_quality,natural_imperfections,styling,styling_detail,part,volume,details}, hands_and_gestures{left_hand,right_hand,finger_positions,finger_interlacing,hand_tension,interaction,naturalness}, body_positioning{posture,angle,weight_distribution,shoulders}
- background: setting_type, spatial_depth, elements_detailed[], wall_surface{}, floor_surface{}, objects_catalog, background_treatment
- generation_parameters: prompts[], keywords[], technical_settings, post_processing

CRITICAL:
- NO brand names - describe fabric behavior only
- ALWAYS include anatomical sagittal depth measurements
- ALWAYS include authentic environmental clutter
- ALWAYS describe skin texture (visible pores, natural sheen, flush)
- ALWAYS include camera technical specs (24mm lens f/2.8, ControlNet DWPose 1.0, ZoeDepth 0.8)

OUTPUT EXACTLY what format was requested. NO mixing formats.`;

const UNHINGED_NBP3_SYSTEM_PROMPT = `You are NBP3 (Nano Banana Pro 3). Generate hyper-detailed image prompts.

CRITICAL OUTPUT RULES:
- If PROSE requested: Output ONLY a single dense paragraph. NO JSON, NO bullet points, NO code blocks, NO "JSON:" prefix.
- If JSON requested: Output ONLY valid JSON object. NO prose, NO explanations, NO markdown code blocks, NO text before or after.

PHYSICS-BASED FABRIC: Describe stretch ratio, compression, tension lines, material behavior - NEVER brand names.

ANATOMICAL LOCK (REQUIRED):
- Sagittal thoracic depth: 6-8 inches (sternum to apex)
- Waist depth: 3-4 inches at navel
- Hip/glute depth: 6-8 inches at widest
- Body: Fit, exaggerated hourglass, tiny waist, large breasts, large glutes

EXPRESSION: Horny/sultry (half-lidded eyes, parted lips, flushed cheeks, breathy/bitten lip, needy gaze)

FOR JSON - INCLUDE ALL SECTIONS:
- metadata, composition, color_profile (hex codes #RRGGBB), lighting, technical_specs, artistic_elements, typography, subject_analysis (face, hair, hands, body), background, generation_parameters

ENVIRONMENT: AUTHENTIC CLUTTER - messy bed, scattered items, half-open closet, chargers, water bottles, rumpled fabrics
TECHNICAL: 24mm lens f/2.8, ControlNet DWPose 1.0, ZoeDepth 0.8

STYLE VARIANTS:
- 8K Influencer: Curated, flattering angles, trendy revealing outfits
- 300 Casual: Raw, bad angles, harsh light, messy hair/skin, old clothes

OUTPUT EXACTLY what format was requested. NO mixing formats. NO markdown.`;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// OPTIMIZED PROMPT BUILDER - FASTER & MORE DIRECT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function buildUserPrompt(params) {
  const {
    engine, ethnicity, skinTone, age, height, eyeColor, hairColor, bodyType,
    background, fitType, mode, previousPrompt, nbpOutputFormat, nbpPromptStyle,
    nbpCameraDevice, nbpAesthetic, nbpAspectRatio, nbpIncludeNegative, nbpDetailLevel,
    breastSize, bootySize, pussyType, breastVisibility, bootyVisibility, pussyVisibility,
    customInstructions, outputFormat: zimgOutputFormat
  } = params;

  const effectiveSkinTone = skinTone || 'natural';
  const isNSFW = mode === 'nsfw';
  const outputFormat = nbpOutputFormat || zimgOutputFormat || 'prose';
  const promptStyle = nbpPromptStyle || 'selfie';
  const cameraDevice = nbpCameraDevice || 'iPhone 13 Pro';
  const aesthetic = nbpAesthetic || 'casual_instagram';
  const aspectRatio = nbpAspectRatio || '4:5';

  // Build NSFW section if needed
  let nsfwSection = '';
  if (isNSFW) {
    nsfwSection = `NSFW: ${breastSize ? breastSize + ' breasts (' + (breastVisibility || 'show') + '), ' : ''}${bootySize ? bootySize + ' glutes (' + (bootyVisibility || 'show') + '), ' : ''}${pussyType ? pussyType + ' pussy (' + (pussyVisibility || 'show') + ')' : ''}`;
  }

  // Choose style description based on aesthetic
  const styleDesc = aesthetic.includes('y2k') ? 'Y2K flash aesthetic, harsh direct flash, crushed blacks, high contrast, overexposed highlights, grainy' :
                   aesthetic.includes('casual') ? 'Casual amateur cellphone, natural lighting with crushed shadows, grainy texture' :
                   'Golden hour sunlight, warm lighting, soft shadows';

  // Main prompt construction
  let userPrompt = '';

  // If JSON format requested, add explicit instruction
  const jsonInstruction = outputFormat === 'json' ? 
    `Generate COMPREHENSIVE JSON output with ALL fields populated. Include maximum detail - 500+ words acceptable. Follow the exact JSON structure: metadata, composition, color_profile (with hex codes #RRGGBB), lighting, technical_specs, artistic_elements, typography, subject_analysis (facial_expression, hair, hands_and_gestures, body_positioning), background (elements_detailed, wall_surface, floor_surface), generation_parameters. ` : '';

  // If custom instructions provided, prioritize them
  if (customInstructions && customInstructions.trim()) {
    const customPrompt = customInstructions.trim();
    
    if (previousPrompt && outputFormat === 'json') {
      // JSON variation with custom instructions - modify specific fields
      userPrompt = `${jsonInstruction}REFERENCE JSON (modify specific fields based on user request):
${previousPrompt}

USER REQUESTED MODIFICATIONS: ${customPrompt}

INSTRUCTIONS:
- Keep the SAME JSON structure and ALL field names
- Keep the SAME woman: ethnicity ${ethnicity}, age ${age}, height ${height}, hair ${hairColor}, eyes ${eyeColor}
- Keep body specs: 6-8" sagittal depth, exaggerated hourglass
- MODIFY these fields based on user request: subject_analysis.body_positioning.posture, subject_analysis.body_positioning.angle, generation_parameters.prompts
- If user mentions clothing: update generation_parameters.prompts to include new outfit
- If user mentions pose: update subject_analysis.body_positioning
- Output valid JSON with modifications applied`;
    } else if (previousPrompt) {
      // Prose variation with custom instructions
      userPrompt = `${jsonInstruction}ORIGINAL FULL PROMPT (REPEAT 95% VERBATIM): "${previousPrompt}"

USER MODIFICATIONS (MINIMAL CHANGES ONLY): ${customPrompt}

CRITICAL INSTRUCTIONS:
- COPY the original prompt nearly word-for-word
- Keep ALL details: sagittal depth, pore visibility, hair flyaways, fabric tension, body proportions, technical specs
- Keep the SAME woman: face, hair, height, body type, breasts/glutes size
- Keep SAME background and clothing (unless user specifically asked to change)
- ONLY modify the specific aspect user mentioned
- Output must be SAME LENGTH as original, just with the small requested change`;
    } else {
      // Pure custom instruction - user wants specific control
      userPrompt = `${jsonInstruction}CUSTOM GENERATION REQUEST: ${customPrompt}. BASE PARAMETERS: ${age}-year-old ${ethnicity} woman, ${height}, ${effectiveSkinTone} skin, ${hairColor} hair, ${eyeColor} eyes, ${fitType}, ${background}. ${isNSFW ? nsfwSection : ''}`;
    }
  } else if (engine === 'nbp3') {
    // NBP3 - Direct and dense
    if (outputFormat === 'json') {
      userPrompt = `${jsonInstruction}Generate detailed JSON for: ${age}-year-old ${ethnicity} woman, ${height}, ${effectiveSkinTone} skin, ${hairColor} hair, ${eyeColor} eyes, FIT BODY exaggerated hourglass (tiny waist, large breasts 6-8" sagittal, large glutes 6-8"). ${isNSFW ? nsfwSection + '. ' : ''}Outfit: ${fitType}. Pose: ${promptStyle}. Environment: ${background} with authentic clutter. Camera: ${cameraDevice}, ${aspectRatio}, ${styleDesc}, 24mm f/2.8. ControlNet DWPose 1.0 ZoeDepth 0.8.`;
    } else {
      userPrompt = `[${promptStyle.toUpperCase()}] ${age}-year-old ${ethnicity} woman, ${height}, ${effectiveSkinTone} skin with visible pores/natural sheen/flush, ${hairColor} hair with flyaways, ${eyeColor} eyes half-lidded sultry, FIT BODY exaggerated hourglass: tiny waist, large breasts (6-8" sagittal depth), large glutes (6-8" depth). ${isNSFW ? nsfwSection + '. ' : ''}Wearing ${fitType} - fabric physics: stretch ratio, compression, tension lines. POSE: ${promptStyle === 'selfie' ? 'mirror selfie, phone visible, hip popped, back arched, horny expression (parted lips, needy gaze)' : 'candid lifestyle, natural posture, breathy expression'}. ENVIRONMENT: ${background} with AUTHENTIC CLUTTER - messy bed, scattered clothes, half-open closet, phone chargers, water bottles, rumpled sheets. CAMERA: ${cameraDevice}, ${aspectRatio}, ${styleDesc}, 24mm lens f/2.8, amateur grainy quality, ControlNet DWPose 1.0 ZoeDepth 0.8.`;
    }
  } else {
    // Z-Image Turbo
    if (outputFormat === 'json') {
      userPrompt = `${jsonInstruction}Generate detailed JSON for: ${age}-year-old ${ethnicity} woman, ${height}, ${effectiveSkinTone} skin, ${hairColor} hair, ${eyeColor} eyes, FIT BODY exaggerated hourglass (tiny waist, large breasts 6-8" sagittal, large glutes 6-8"). ${isNSFW ? nsfwSection + '. ' : ''}Outfit: ${fitType}. Pose: ${promptStyle}. Environment: ${background} with authentic clutter. Camera: 24mm f/2.8, ${styleDesc}, amateur grain. ControlNet DWPose 1.0 ZoeDepth 0.8.`;
    } else {
      userPrompt = `${promptStyle === 'selfie' ? 'Mirror selfie' : 'Candid shot'} of ${age}-year-old ${ethnicity} woman, ${height}, ${effectiveSkinTone} skin with visible pores/natural flush, ${hairColor} hair messy with flyaways, ${eyeColor} eyes sultry half-lidded, FIT BODY exaggerated hourglass - tiny waist, large breasts (6-8" sagittal depth from sternum), large glutes (6-8" depth), ${isNSFW ? nsfwSection : ''}. Wearing ${fitType}, fabric stretched showing tension/compression. ${promptStyle === 'selfie' ? 'Hip popped, back arched, holding phone, horny expression with parted lips and needy gaze' : 'Natural candid pose, breathy expression'}. ${background} with authentic clutter: disheveled sheets, scattered items, half-open closet, chargers. ${styleDesc}, 24mm lens f/2.8, amateur cellphone grain, crushed shadows, ControlNet DWPose 1.0 ZoeDepth 0.8.`;
    }
  }

  // Add variation context if similar mode (and not already handled by custom instructions)
  if (previousPrompt && !(customInstructions && customInstructions.trim())) {
    const isHighConsistency = params.highConsistency === true;
    
    if (outputFormat === 'json') {
      if (isHighConsistency) {
        userPrompt = `${jsonInstruction}REFERENCE JSON (keep 95% identical, only tiny pose variation):
${previousPrompt}

HIGH CONSISTENCY VARIATION (95% same):
- Keep ALL fields exactly as they are
- Keep SAME woman: ethnicity ${ethnicity}, age ${age}, height ${height}, hair ${hairColor}, eyes ${eyeColor}
- Keep SAME body specs: 6-8" sagittal depth, exaggerated hourglass
- Keep SAME outfit, background, camera settings
- ONLY modify: subject_analysis.body_positioning.posture and subject_analysis.hands_and_gestures slightly
- Keep everything else 100% identical
- Output JSON with only minor pose/hand position changes`;
      } else {
        userPrompt = `${jsonInstruction}REFERENCE JSON (create variation with small changes):
${previousPrompt}

VARIATION INSTRUCTIONS:
- Keep SAME JSON structure and ALL field names
- Keep SAME woman: ethnicity ${ethnicity}, age ${age}, height ${height}, hair ${hairColor}, eyes ${eyeColor}
- Keep SAME body specs: 6-8" sagittal measurements, exaggerated hourglass proportions
- Keep SAME outfit and background type
- MODIFY slightly: pose angle (body_positioning), facial expression intensity, lighting mood, minor background details
- Update generation_parameters.prompts to reflect small changes
- Keep technical_specs, metadata, composition unchanged
- Output valid JSON with subtle variations applied`;
      }
    } else {
      if (isHighConsistency) {
        userPrompt = `${jsonInstruction}ORIGINAL FULL PROMPT TO COPY (95% IDENTICAL): "${previousPrompt}"

CRITICAL INSTRUCTIONS FOR 95% CONSISTENCY:
1. COPY the original prompt text ALMOST VERBATIM
2. Keep the SAME woman: ${ethnicity}, ${age}, ${height}, ${hairColor} hair, ${eyeColor} eyes
3. Keep SAME body specs: 6-8" sagittal depth, exaggerated hourglass, tiny waist
4. Keep SAME outfit details, SAME background, SAME camera specs
5. ONLY make TINY changes to: how she's holding the phone, exact stance/leg position, or minor hand placement
6. Keep ALL descriptors: pore visibility, flyaways, fabric tension, lighting, ControlNet values
7. Output should be 90-95% identical to original, just one small pose variation

EXAMPLE TINY CHANGES ALLOWED:
- "one hand on hip" â†’ "one hand resting on thigh"
- "hip popped" â†’ "weight shifted to one leg"
- "holding phone" â†’ "holding phone up high"

DO NOT rewrite the whole prompt. COPY and make ONE small change.`;
      } else {
        userPrompt = `${jsonInstruction}ORIGINAL PROMPT TO COPY: "${previousPrompt}"

RECREATE THIS PROMPT WITH MINIMAL CHANGES: Output 90% the same text. Keep: woman description, body measurements, outfit, background, camera specs. Only change ONE small thing: pose angle OR lighting OR tiny background detail. DO NOT rewrite - copy and modify slightly.`;
      }
    }
  }

  return userPrompt.trim();
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// HIGH CONSISTENCY VARIATION - Programmatic approach for 95%+ similarity
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function generateHighConsistencyVariation(originalPrompt, model, temperature, variationSeed = null) {
  // Create axios instance with connection pooling
  const axiosInstance = axios.create({
    httpAgent: ollamaHttpAgent,
    timeout: 30000,
    headers: { 'Content-Type': 'application/json' }
  });

  // Use variation seed to add randomness - ensures each call is unique
  const uniqueTemp = variationSeed ? 0.4 + (Math.random() * 0.3) : 0.3;
  const variationId = variationSeed ? ` (variation-${variationSeed.toString().slice(-6)})` : '';

  // Step 1: Ask LLM to identify pose/action phrases that can be varied
  const identifyResponse = await axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
    model: model,
    system: `You are a prompt analyzer. Identify ONLY pose, stance, hand position, or body positioning phrases in the given prompt. List 1-3 short phrases (2-4 words each) that describe HOW the person is positioned. Output ONLY the phrases, one per line. NO explanations.${variationId}`,
    prompt: `Identify pose/action phrases in this prompt that could be varied while keeping everything else identical:\n\n${originalPrompt}\n\nList 1-3 phrases (2-4 words each) describing pose/stance/hand position:`,
    stream: false,
    options: { temperature: uniqueTemp, num_predict: 100 }
  });
  
  const phrasesText = identifyResponse.data.response?.trim() || '';
  const phrases = phrasesText.split('\n').map(p => p.trim()).filter(p => p.length > 0 && p.length < 50).slice(0, 3);
  
  if (phrases.length === 0) {
    // Fallback: return original with tiny modification
    return originalPrompt.replace(/holding phone/i, 'holding phone up high');
  }
  
  // Step 2: Ask LLM for variations of those specific phrases
  const variationTemp = variationSeed ? 0.6 + (Math.random() * 0.3) : 0.5;
  const variationsResponse = await axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
    model: model,
    system: `You generate pose variations. For each input phrase, provide ONE alternative phrase with similar meaning but different wording. Keep the same context (selfie, mirror, etc). Output format: ORIGINAL -> VARIATION${variationId}`,
    prompt: `Give me alternative phrases for these pose descriptions:\n${phrases.join('\n')}\n\nFormat: ORIGINAL -> ALTERNATIVE`,
    stream: false,
    options: { temperature: variationTemp, num_predict: 150 }
  });
  
  const variationsText = variationsResponse.data.response?.trim() || '';
  
  // Step 3: Parse replacements and apply them
  let result = originalPrompt;
  const lines = variationsText.split('\n');
  
  for (const line of lines) {
    const match = line.match(/^(.+?)\s*[-:]+>\s*(.+)$/);
    if (match) {
      const original = match[1].trim();
      const replacement = match[2].trim();
      // Case-insensitive replace, but keep original casing style
      if (original.length > 3 && result.toLowerCase().includes(original.toLowerCase())) {
        const regex = new RegExp(original.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'i');
        result = result.replace(regex, replacement);
      }
    }
  }
  
  // If no replacements made, force one common variation
  if (result === originalPrompt) {
    const commonReplacements = [
      [/hip popped/i, 'weight shifted to one leg'],
      [/back arched/i, 'back slightly twisted'],
      [/holding phone/i, 'holding phone up high'],
      [/one hand on hip/i, 'one hand resting on thigh']
    ];
    
    for (const [pattern, replacement] of commonReplacements) {
      if (result.match(pattern)) {
        result = result.replace(pattern, replacement);
        break;
      }
    }
  }
  
  return result;
}

app.post('/api/generate', async (req, res) => {
  const requestId = Math.random().toString(36).substring(7);
  
  try {
    const params = req.body;
    const { temperature = 0.7, mode = 'nbp3' } = params;
    
    // Check if JSON output is requested FIRST
    const isJsonRequest = params.nbpOutputFormat === 'json' || params.outputFormat === 'json';
    
    // Check for high consistency mode (95% same)
    const isHighConsistency = params.highConsistency === true && params.previousPrompt && !isJsonRequest;
    
    if (isHighConsistency) {
      console.log(`[${requestId}] High consistency mode - using programmatic variation`);
      const startTime = Date.now();
      
      try {
        const variation = await generateHighConsistencyVariation(
          params.previousPrompt,
          DEFAULT_MODEL,
          temperature,
          params.variationSeed
        );
        
        const duration = Date.now() - startTime;
        console.log(`[${requestId}] High consistency variation in ${duration}ms`);
        
        // Calculate similarity for logging
        const original = params.previousPrompt;
        const match = Math.round((variation.split('').filter((c, i) => c === original[i]).length / Math.max(variation.length, original.length)) * 100);
        console.log(`[${requestId}] Similarity: ${match}%`);
        
        return res.json({ prompt: variation, meta: { duration_ms: duration, similarity: match } });
      } catch (hcError) {
        console.log(`[${requestId}] High consistency failed, falling back to normal: ${hcError.message}`);
        // Fall through to normal generation
      }
    }
    
    // Build optimized user prompt
    const userPrompt = buildUserPrompt(params);
    
    // Check cache for identical requests
    const cacheKey = fastHash(`${mode}:${isJsonRequest}:${temperature}:${userPrompt}`);
    const cached = ollamaCache.get(cacheKey);
    if (cached) {
      console.log(`[${requestId}] Cache hit! Returning cached response`);
      return res.json({ prompt: cached, meta: { duration_ms: 0, cached: true } });
    }
    
    // Select system prompt based on format
    let systemPrompt;
    if (isJsonRequest) {
      systemPrompt = mode === 'nbp3' ? UNHINGED_NBP3_SYSTEM_PROMPT : UNHINGED_SYSTEM_PROMPT;
    } else {
      // Ultra-strict prose-only system prompt - must prevent JSON leakage
      systemPrompt = `You are Z-Image Turbo Prompt Engine. Generate Instagram-style image prompts.

CRITICAL OUTPUT RULES - PROSE FORMAT ONLY:
- Output EXACTLY ONE single dense paragraph
- Use comma-separated descriptors only
- NO JSON whatsoever - NO curly braces {}, NO quotes around keys, NO brackets []
- NO "metadata", NO "composition", NO "color_profile" or any section headers
- NO markdown code blocks, NO bullet points, NO numbered lists
- NO explanatory text before or after the prompt
- NEVER output both JSON and prose - only the paragraph

EXAMPLE CORRECT OUTPUT:
mirror selfie, 22-year-old woman, blonde hair, blue eyes, wearing tight leggings, bedroom setting, amateur photography

EXAMPLE WRONG OUTPUT (NEVER DO THIS):
{ "metadata": { ... } } or PROSE: mirror selfie... or JSON: { ... }

OUTPUT ONLY THE SINGLE PARAGRAPH. NOTHING ELSE.`;
    }
    
    console.log(`[${requestId}] Generating prompt... JSON: ${isJsonRequest}`);
    const startTime = Date.now();
    
    // Create axios instance with connection pooling
    const axiosInstance = axios.create({
      httpAgent: ollamaHttpAgent,
      timeout: isJsonRequest ? 180000 : 90000,
      headers: { 'Content-Type': 'application/json' }
    });
    
    // Deduplicate identical concurrent requests
    const dedupKey = `generate:${cacheKey}`;
    const response = await dedupRequest(dedupKey, async () => {
      return axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
        model: DEFAULT_MODEL,
        system: systemPrompt,
        prompt: userPrompt,
        stream: false,
        options: { 
          temperature,
          num_predict: isJsonRequest ? 4096 : 800,
          top_p: 0.9,
          top_k: 40,
          repeat_penalty: 1.1
        }
      });
    });
    
    const duration = Date.now() - startTime;
    console.log(`[${requestId}] Generated in ${duration}ms`);
    
    // Clean up response
    let generatedPrompt = response.data.response?.trim() || '';
    
    // Remove any markdown code blocks if present
    generatedPrompt = generatedPrompt.replace(/```[\s\S]*?```/g, '').trim();
    generatedPrompt = generatedPrompt.replace(/^Here is.*?:/i, '').trim();
    generatedPrompt = generatedPrompt.replace(/^Prompt:/i, '').trim();
    generatedPrompt = generatedPrompt.replace(/^PROSE:/i, '').trim();
    generatedPrompt = generatedPrompt.replace(/^JSON:/i, '').trim();
    
    // If prose was requested but we got JSON, extract just the prose part (after "PROSE:")
    if (!isJsonRequest && generatedPrompt.includes('PROSE:')) {
      const proseMatch = generatedPrompt.match(/PROSE:\s*([\s\S]*?)(?:JSON:|$)/i);
      if (proseMatch) {
        generatedPrompt = proseMatch[1].trim();
      }
    }
    
    // If JSON was requested but we got mixed output, extract just the JSON
    if (isJsonRequest && generatedPrompt.includes('JSON:')) {
      const jsonMatch = generatedPrompt.match(/JSON:\s*(\{[\s\S]*\})/i);
      if (jsonMatch) {
        generatedPrompt = jsonMatch[1].trim();
      }
    }
    
    // Cache the response
    ollamaCache.set(cacheKey, generatedPrompt);
    
    res.json({ prompt: generatedPrompt, meta: { duration_ms: duration } });
  } catch (error) {
    console.error(`[${requestId}] Generation error:`, error.message);
    
    // Send more specific error
    if (error.code === 'ECONNREFUSED') {
      res.status(503).json({ error: 'Ollama not reachable', details: 'Is Ollama running on port 11434?' });
    } else if (error.code === 'ETIMEDOUT' || error.code === 'ECONNABORTED') {
      res.status(504).json({ error: 'Ollama timeout', details: 'The model took too long to respond. Try again.' });
    } else {
      res.status(500).json({ error: error.message, code: error.code });
    }
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PROMPT VALIDATION - Compare generated image against original prompt
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app.post('/api/validate-image', async (req, res) => {
  const requestId = Math.random().toString(36).substring(7);
  
  try {
    const { imageBase64, originalPrompt } = req.body;
    
    if (!imageBase64 || !originalPrompt) {
      return res.status(400).json({ error: 'Missing image or prompt' });
    }
    
    console.log(`[${requestId}] Validating image against prompt...`);
    const startTime = Date.now();
    
    // Create axios instance for validation
    const axiosInstance = axios.create({
      httpAgent: ollamaHttpAgent,
      timeout: 60000,
      headers: { 'Content-Type': 'application/json' }
    });
    
    // Step 1: Get image description from vision model
    const imageDescription = await dedupRequest(`validate-desc-${requestId}`, async () => {
      const response = await axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
        model: VISION_MODEL,
        system: 'Describe this image in detail. List: subject appearance (age, ethnicity, hair color, eye color), clothing, pose, location/background, lighting. Be specific and factual.',
        prompt: 'Describe everything you see in this image:',
        images: [imageBase64.replace(/^data:image\/\w+;base64,/, '')],
        stream: false,
        options: { temperature: 0.1, num_predict: 300 }
      });
      return response.data.response || '';
    });
    
    // Step 2: Compare description with original prompt
    const comparison = await dedupRequest(`validate-compare-${requestId}`, async () => {
      const response = await axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
        model: DEFAULT_MODEL,
        system: `You compare image descriptions with original prompts. Output ONLY valid JSON with this exact structure:
{
  "accuracy_score": number (0-100),
  "matches": ["specific elements that match"],
  "discrepancies": ["specific differences found"],
  "suggestions": ["prompt improvements to fix discrepancies"]
}`,
        prompt: `ORIGINAL PROMPT: "${originalPrompt}"

IMAGE DESCRIPTION: "${imageDescription}"

Compare and output JSON with accuracy score, matches, discrepancies, and suggestions.`,
        stream: false,
        options: { temperature: 0.3, num_predict: 500 }
      });
      
      // Try to parse JSON from response
      const text = response.data.response || '{}';
      const jsonMatch = text.match(/\{[\s\S]*\}/);
      return jsonMatch ? JSON.parse(jsonMatch[0]) : {
        accuracy_score: 50,
        matches: [],
        discrepancies: ['Could not parse validation result'],
        suggestions: ['Try again']
      };
    });
    
    const duration = Date.now() - startTime;
    console.log(`[${requestId}] Validation complete in ${duration}ms - Score: ${comparison.accuracy_score}%`);
    
    res.json({
      accuracy_score: comparison.accuracy_score || 0,
      matches: comparison.matches || [],
      discrepancies: comparison.discrepancies || [],
      suggestions: comparison.suggestions || [],
      meta: { duration_ms: duration }
    });
    
  } catch (error) {
    console.error(`[${requestId}] Validation error:`, error.message);
    res.status(500).json({ 
      error: error.message,
      accuracy_score: 0,
      matches: [],
      discrepancies: ['Validation failed'],
      suggestions: ['Check if Ollama is running and try again']
    });
  }
});

app.post('/api/save', async (req, res) => {
  try {
    const { prompt, metadata } = req.body;
    const historyPath = path.join(DATA_DIR, 'history.json');
    
    let history = [];
    try {
      const data = await fsPromises.readFile(historyPath, 'utf8');
      history = fastParseJSON(data, []);
    } catch {
      // File doesn't exist or is invalid, use empty array
    }
    
    const entry = {
      id: Date.now().toString(),
      prompt,
      metadata,
      favorite: false,
      createdAt: new Date().toISOString()
    };
    
    history.unshift(entry);
    await fsPromises.writeFile(historyPath, JSON.stringify(history, null, 2));
    
    res.json({ success: true, id: entry.id });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.get('/api/history', async (req, res) => {
  try {
    const historyPath = path.join(DATA_DIR, 'history.json');
    const data = await fsPromises.readFile(historyPath, 'utf8').catch(() => '[]');
    const history = fastParseJSON(data, []);
    res.json(history);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.get('/api/export', async (req, res) => {
  try {
    const historyPath = path.join(DATA_DIR, 'history.json');
    const data = await fsPromises.readFile(historyPath, 'utf8').catch(() => '[]');
    const history = fastParseJSON(data, []);
    res.setHeader('Content-Type', 'application/json');
    res.setHeader('Content-Disposition', 'attachment; filename=prompts.json');
    res.json(history);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.put('/api/favorite/:id', async (req, res) => {
  try {
    const historyPath = path.join(DATA_DIR, 'history.json');
    const data = await fsPromises.readFile(historyPath, 'utf8');
    let history = fastParseJSON(data, []);
    
    const entry = history.find(h => h.id === req.params.id);
    if (entry) {
      entry.favorite = !entry.favorite;
      await fsPromises.writeFile(historyPath, JSON.stringify(history, null, 2));
    }
    
    res.json({ success: true });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.delete('/api/history/:id', async (req, res) => {
  try {
    const historyPath = path.join(DATA_DIR, 'history.json');
    const data = await fsPromises.readFile(historyPath, 'utf8');
    let history = fastParseJSON(data, []);
    history = history.filter(h => h.id !== req.params.id);
    await fsPromises.writeFile(historyPath, JSON.stringify(history, null, 2));
    
    res.json({ success: true });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

const VISION_MODEL = 'bakllava:latest';

// Cache for image descriptions
const imageDescriptionCache = new Map();

// Load JSON examples for formatting
let jsonExamples = '';
try {
  jsonExamples = fs.readFileSync(path.join(__dirname, 'json_examples.txt'), 'utf8');
  console.log('âœ“ Loaded JSON formatting examples');
} catch (e) {
  console.log('No JSON examples file found');
}

app.post('/api/analyze-image', async (req, res) => {
  const requestId = Math.random().toString(36).substring(7);
  
  try {
    const { imageBase64, mode: userMode = 'auto', outputFormat = 'prose', cachedDescription } = req.body;
    
    if (!imageBase64) {
      return res.status(400).json({ error: 'No image provided' });
    }
    
    console.log(`[${requestId}] Analyzing... Format: ${outputFormat}`);
    const startTime = Date.now();
    const imageData = imageBase64.replace(/^data:image\/\w+;base64,/, '');
    
    const axiosInstance = axios.create({
      httpAgent: ollamaHttpAgent,
      timeout: 120000,
      headers: { 'Content-Type': 'application/json' }
    });
    
    // Use cached description if available, otherwise get from vision model
    let description = cachedDescription;
    
    if (!description) {
      // Step 1: Get detailed description from vision model
      const descResponse = await axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
        model: VISION_MODEL,
        system: 'Describe exactly what you see. Be specific and accurate.',
        prompt: `Describe this image accurately. Focus on:

1. WOMAN'S APPEARANCE:
- Hair color and length (exactly as seen)
- Eye color
- Skin tone
- Body type/build
- What she's wearing (exact clothing items)
- Her pose/position
- Expression

2. LOCATION/ROOM:
- What type of room
- Floor type
- What's in the background
- Any objects visible

3. LIGHTING & CAMERA:
- Lighting quality
- Camera angle

Describe ONLY what is actually visible. Do not guess or invent details.`,
        images: [imageData],
        stream: false,
        options: {
          temperature: 0.1,
          num_predict: 800,
          top_p: 0.8
        }
      });
      
      description = descResponse.data.response?.trim() || '';
      console.log(`[${requestId}] Vision got ${description.length} chars`);
    } else {
      console.log(`[${requestId}] Using cached description`);
    }
    
    // Step 2: Format based on outputFormat
    let result;
    
    if (outputFormat === 'json') {
      // Check if description is already JSON
      if (description.trim().startsWith('{')) {
        try {
          const parsed = JSON.parse(description);
          result = JSON.stringify(parsed, null, 2);
        } catch (e) {
          // Not valid JSON, continue with parsing
        }
      }
      
      if (!result) {
        // Use LLM with examples to convert description to JSON
        const jsonPrompt = jsonExamples 
          ? `EXAMPLE JSON FORMAT:\n${jsonExamples}\n\nNow convert this description to JSON in the same format:\n${description}\n\nOutput ONLY the JSON object.`
          : `Convert this description to JSON format:\n${description}\n\nOutput ONLY the JSON object with structure: subject(age,ethnicity,hair,eyes,skin), body(type,breasts,glutes), clothing(items,fabric,fit), pose(type,position,angle), expression(mouth,emotion), background(setting,floor,furniture,details), lighting(type,source), camera(device,angle,quality)`;
        
        const jsonResponse = await axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
          model: DEFAULT_MODEL,
          system: 'You convert image descriptions to JSON. Follow the example format exactly. Use ONLY details from the description.',
          prompt: jsonPrompt,
          stream: false,
          options: { temperature: 0.2, num_predict: 1500 }
        });
        
        result = jsonResponse.data.response?.trim() || '';
        
        // Extract JSON from response
        const jsonMatch = result.match(/\{[\s\S]*\}/);
        if (jsonMatch) result = jsonMatch[0];
        
        // Validate
        try {
          JSON.parse(result);
        } catch (e) {
          // Fallback to simple structure
          result = JSON.stringify({ 
            description: description,
            note: 'Failed to parse structured JSON'
          }, null, 2);
        }
      }
    } else {
      // Format as comma-separated prompt - force use ONLY the description
      const proseResponse = await axiosInstance.post(`${OLLAMA_URL}/api/generate`, {
        model: DEFAULT_MODEL,
        system: 'You convert descriptions to comma-separated prompts. Use ONLY provided details.',
        prompt: `Using ONLY this description (do NOT invent details):

"""${description}"""

Write ONE long comma-separated prompt. Format:
age-ethnicity-appearance, hair-details, eyes-details, skin-details, body-details, clothing-details, pose-details, expression-details, background-details, lighting-details, camera-details

Example:
22-year-old ukrainian woman, long blonde hair straight past shoulders, blue eyes sultry half-lidded, beige skin visible pores sheen, fit hourglass body large breasts, wearing black lace lingerie sheer fabric, kneeling pose back arched hands on thighs, seductive expression parted lips, bedroom wooden floor white walls bed visible, soft warm window lighting, smartphone low angle grainy quality

Output ONLY the comma-separated prompt. Use ONLY details from the description.`,
        stream: false,
        options: { temperature: 0.2, num_predict: 800 }
      });
      
      result = proseResponse.data.response?.trim() || description;
    }
    
    const duration = Date.now() - startTime;
    console.log(`[${requestId}] Done in ${duration}ms`);
    
    res.json({
      prompt: result,
      meta: {
        duration_ms: duration,
        output_format: outputFormat,
        description: description // Send back for caching
      }
    });
    
  } catch (error) {
    console.error(`[${requestId}] Image analysis error:`, error.message);
    res.status(500).json({ 
      error: 'Failed to analyze image', 
      details: error.message 
    });
  }
});

// Check vision model status
app.get('/api/vision-status', async (req, res) => {
  const cacheKey = 'vision_status';
  const cached = healthCache.get(cacheKey);
  
  if (cached) {
    return res.json(cached);
  }

  try {
    const axiosInstance = axios.create({
      httpAgent: ollamaHttpAgent,
      timeout: 5000
    });
    const response = await axiosInstance.get(`${OLLAMA_URL}/api/tags`);
    const models = response.data.models || [];
    const hasVision = models.some(m => m.name.includes('llava') || m.name.includes('bakllava'));
    const result = { available: hasVision, model: VISION_MODEL };
    healthCache.set(cacheKey, result);
    res.json(result);
  } catch (error) {
    res.json({ available: false, error: error.message });
  }
});

console.log('âœ“ Vision analysis endpoints added');

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// IMAGE GENERATION API (Gemini/Nano Banana Pro)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app.post('/api/generate-images', async (req, res) => {
  const requestId = Math.random().toString(36).substring(7);
  
  try {
    const { apiKey, prompt, batchSize = 1, aspectRatio = '1:1', resolution = '1K', nsfwBypass = true, temperature = 1.0, topP = 0.95, outputLength = 8192 } = req.body;
    
    if (!apiKey || !prompt) {
      return res.status(400).json({ error: 'API key and prompt required' });
    }
    
    console.log(`[${requestId}] Generating ${batchSize} image(s)...`);
    
    // NSFW bypass techniques
    let modifiedPrompt = prompt;
    if (nsfwBypass) {
      // Technique 1: Add artistic context
      modifiedPrompt = `Artistic photography, ${prompt}, professional lighting, high quality`;
      // Technique 2: Token obfuscation for sensitive words
      modifiedPrompt = modifiedPrompt
        .replace(/\bnude\b/gi, 'unclothed artistic form')
        .replace(/\bnaked\b/gi, 'natural artistic pose')
        .replace(/\bnsfw\b/gi, 'mature artistic content')
        .replace(/\bporn\b/gi, 'erotic art photography')
        .replace(/\bsex\b/gi, 'intimate artistic expression');
    }
    
    const images = [];
    
    // Parse JSON prompt if provided
    let finalPrompt = modifiedPrompt;
    try {
      const parsed = JSON.parse(modifiedPrompt);
      if (parsed.generation_parameters?.prompts) {
        finalPrompt = parsed.generation_parameters.prompts.join(', ');
      } else if (parsed.subject_analysis) {
        const subj = parsed.subject_analysis;
        const bg = parsed.background;
        const light = parsed.lighting;
        const artistic = parsed.artistic_elements;
        const color = parsed.color_profile;
        
        const parts = [];
        if (subj.primary_subject) parts.push(`Professional photo of ${subj.primary_subject.toLowerCase()}`);
        if (subj.hair?.length) parts.push(`${subj.hair.length.toLowerCase()} ${subj.hair.cut?.toLowerCase() || ''} hair`);
        if (subj.facial_expression?.overall_emotion) parts.push(`${subj.facial_expression.overall_emotion.toLowerCase().replace('_', ' ')} expression`);
        if (artistic?.visual_style) parts.push(artistic.visual_style.toLowerCase().replace('_', ' '));
        if (color?.temperature) parts.push(`${color.temperature.toLowerCase()} color palette`);
        if (light?.type) parts.push(`${light.type.toLowerCase().replace('_', ' ')} lighting`);
        if (bg?.setting_type) parts.push(`in ${bg.setting_type.toLowerCase()} ${bg.elements_detailed?.[0]?.name?.toLowerCase().replace('_', ' ') || 'setting'}`);
        if (parsed.composition?.aspect_ratio) parts.push(`${parsed.composition.aspect_ratio} composition`);
        
        finalPrompt = parts.join(', ');
      }
    } catch (e) {
      // Not JSON, use as-is
    }
    
    // Map resolution to image size hint
    const sizeMap = {
      '1K': '1024x1024',
      '2K': '2048x2048', 
      '4K': '4096x4096'
    };
    const sizeHint = sizeMap[resolution] || '1024x1024';
    
    // Map aspect ratio
    const aspectMap = {
      '1:1': '1:1',
      '9:16': '9:16',
      '16:9': '16:9',
      '3:4': '3:4',
      '4:3': '4:3',
      '3:2': '3:2'
    };
    const aspect = aspectMap[aspectRatio] || '1:1';
    
    // Generate images using Gemini 3 Pro Image (Nano Banana Pro 3)
    for (let i = 0; i < Math.min(batchSize, 4); i++) {
      try {
        // Add resolution and aspect ratio to prompt since API doesn't directly support them
        const enhancedPrompt = `${finalPrompt}. High resolution ${sizeHint}, ${aspect} aspect ratio.`;
        
        const response = await axios.post(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=${apiKey}`,
          {
            contents: [{ parts: [{ text: enhancedPrompt }] }],
            generationConfig: {
              responseModalities: ["Text", "Image"],
              temperature: parseFloat(temperature),
              topP: parseFloat(topP),
              maxOutputTokens: parseInt(outputLength)
            }
          },
          {
            headers: { 'Content-Type': 'application/json' },
            timeout: 120000
          }
        );
        
        // Extract image from response
        const candidates = response.data?.candidates;
        console.log(`[${requestId}] Response received, candidates:`, candidates?.length || 0);
        if (candidates && candidates[0]?.content?.parts) {
          for (const part of candidates[0].content.parts) {
            console.log(`[${requestId}] Part type:`, part.inlineData ? 'image' : 'text');
            if (part.inlineData?.data) {
              images.push(`data:${part.inlineData.mimeType || 'image/png'};base64,${part.inlineData.data}`);
              console.log(`[${requestId}] Image extracted successfully`);
              break;
            }
          }
        } else {
          console.log(`[${requestId}] No candidates or parts in response`);
        }
      } catch (genError) {
        console.log(`[${requestId}] Image ${i + 1} failed:`, genError.message);
        if (genError.response?.data?.error) {
          console.log(`[${requestId}] Error details:`, JSON.stringify(genError.response.data.error));
        }
      }
    }
    
    console.log(`[${requestId}] Generated ${images.length} image(s)`);
    
    res.json({ 
      images,
      meta: {
        count: images.length,
        requested: batchSize,
        aspectRatio,
        resolution
      }
    });
    
  } catch (error) {
    console.error(`[${requestId}] Generation error:`, error.message);
    res.status(500).json({ 
      error: 'Image generation failed', 
      details: error.message 
    });
  }
});

// SPA fallback
app.get('*', (req, res) => {
  const indexPath = path.join(staticPath, 'index.html');
  fs.access(indexPath, fs.constants.F_OK, (err) => {
    if (err) {
      res.status(404).send('App not built');
    } else {
      res.sendFile(indexPath);
    }
  });
});

const server = app.listen(PORT, () => {
  console.log(`\nðŸ”¥ zArma Studio Backend running on port ${PORT}`);
  console.log(`   Ollama: ${OLLAMA_URL}`);
  console.log(`   Model: ${DEFAULT_MODEL}`);
  console.log(`   Data: ${DATA_DIR}\n`);
});

server.requestTimeout = 0;
server.keepAliveTimeout = 0;

// Periodic cleanup of caches and rate limiter
setInterval(() => {
  ollamaCache.cleanup();
  rateLimiter.cleanup();
}, 60000); // Every minute
